## 三种解析数据的方式（主学Xpath）

### 正则表达式（import re）

1. 导入

   ```python
   import re
   ```

   

2. 正则表达式基础

   - 正则表达式由普通字符和特殊字符（元字符）组成

     | 元字符 |                       含义                       |
     | :----: | :----------------------------------------------: |
     |   .    |         匹配除换行符 `\n` 之外的任意字符         |
     |   ^    |                 匹配字符串的开头                 |
     |   $    |                 匹配字符串的结尾                 |
     |   *    |      匹配前一个字符 0 次或多次（贪婪匹配）       |
     |   +    |            匹配前一个字符 1 次或多次             |
     |   ?    |     匹配前一个字符 0 次或 1 次（非贪婪匹配）     |
     |  {n}   |                 精确匹配 `n` 次                  |
     |  {n,}  |                 匹配至少 `n` 次                  |
     | {n,m}  |                匹配 `n` 到 `m` 次                |
     |   []   |        字符集，匹配方括号内的任意一个字符        |
     |   \d   |           匹配任意数字，相当于 `[0-9]`           |
     |   \D   |         匹配非数字字符，相当于 `[^0-9]`          |
     |   \w   |  匹配字母、数字或下划线，相当于 `[a-zA-Z0-9_]`   |
     |   \W   | 匹配非字母、数字或下划线，相当于 `[^a-zA-Z0-9_]` |
     |   \s   |       匹配空白字符（空格、制表符、换行符）       |
     |   \S   |                  匹配非空白字符                  |
     |   ()   |                  组合，用于分组                  |

   

3. Python中re模块的常用函数

   - **re.match()** —— 从字符串起始位置匹配
   - **re.search()** —— 搜索整个字符串，找到第一个匹配项
   - **re.findall()** —— 获取所有匹配项（返回列表）
   - **re.finditer()** —— 迭代器返回所有匹配项（更节省内存）
   - **re.sub()** —— 替换匹配内容（可以匹配完在使用str的函数str.replace来进行数据处理）

   

4. 贪婪（Greedy）与非贪婪（Lazy）匹配

   - 正则默认是 **贪婪匹配**（尽可能匹配更多字符）。要使用 **非贪婪匹配**，在 `*`、`+`、`?` 等符号后加 `?`。

   

5. 如果同一个正则表达式需要多次使用，可以 **预编译** 提高性能。

   - pattern = re.compile(r"\d+")  # 编译正则

   

6. Flags

   |          标志          |                       作用                        |
   | :--------------------: | :-----------------------------------------------: |
   |   `re.S`（`DOTALL`）   |              让 `.` 匹配换行符 `\n`               |
   | `re.I`（`IGNORECASE`） |                    忽略大小写                     |
   | `re.M`（`MULTILINE`）  |            让 `^` 和 `$` 作用于每一行             |
   |  `re.X`（`VERBOSE`）   |          允许使用空格和注释，提高可读性           |
   |   `re.A`（`ASCII`）    |         让 `\w`、`\d`、`\s` 仅匹配 ASCII          |
   |   `re.L`（`LOCALE`）   |            依赖本地语言环境（较少用）             |
   |  `re.U`（`UNICODE`）   | 让 `\w`、`\d`、`\s` 支持 Unicode（Python 3 默认） |
   |        re.DEBUG        |            显示正则解析细节，帮助调试             |



### Xpath（from lxml import etree）

1. 安装与导包

   ```python
   pip install lxml
   
   from lxml import etree
   ```

   

2. 基本路径表达式

   | 语法 |             说明             |             示例             |
   | :--: | :--------------------------: | :--------------------------: |
   |  /   |          选取根节点          |    /html 选取 HTML 根节点    |
   |  //  | 选取所有匹配的节点，不管层级 |      //div选取所有的div      |
   |  .   |         选取当前节点         | ./p选取当前节点下的所有p标签 |
   |  ..  |     选取当前节点的父节点     |       ../ 选取父级元素       |
   |  @   |           选取属性           |     //@href选取所有链接      |

   

3. 节点选取

   - 元素节点

     - //div  # 选取所有 <div> 节点

   - 属性节点

     - //a/@href  # 选取所有 <a> 标签的 href 属性值

   - 文本节点

     - //p/text()  # 选取 <p> 内的文本

   - 通配符

     | 符号 |     作用     |        示例         |
     | :--: | :----------: | :-----------------: |
     |  *   | 选取所有标签 |  //* 选取所有节点   |
     |  @*  | 选取所有属性 | `//@*` 选取所有属性 |

   

4. 逻辑运算

   - |（或）：
     - //h1 | //p  # 选取所有 <h1> 和 <p>：选取多个标签
   - or
     - //a[@class='link'] or //a[@id='special']：满足任意条件
   - and
     - 满足所有条件

   

5. 过滤条件

   - 索引选取

     - //div[1]  # 选取第一个 <div>
     - //p[n] # 选取第n个<p>
     - //li[last()] # 选取最后一个li标签

   - 按属性筛选

     - //a[@href]  # 选取所有带 href 的 <a>
     - //p[@class="text"]

   - 部分匹配

     |     函数      |   作用   |               示例                |
     | :-----------: | :------: | :-------------------------------: |
     |  contains()   |   包含   | `//a[contains(@href, 'example')]` |
     | starts-with() | 以…开头  | //a[starts-with(@href, 'https')]  |
     |    text()     | 选取文本 |     //p[text()='Hello World']     |

   

6. Xpath的例子

   - `//tag[contains(@attr, 'value')]`：部分匹配

     `//tag[starts-with(@attr, 'value')]`：开头匹配

     `//tag[last()]`：选取最后一个

     

### BeautifulSoup(from bs4 import BeautifulSoup)

1. 安装与导入

   ```python
   pip install bs4	/ pip install beautifulsoup4
   
   from bs4 import BeautifulSoup
   
   soup = BeautifulSoup(html, "lxml") # 推荐使用lxml解析，速度较快（需要安装）
   ```

   

2. find()和find_all()方法

   ```python
   # find()：返回第一个匹配的标签
   p_tag = soup.find("p")
   print(p_tag.text)
   
   # find_all()：返回所有匹配的标签
   all_p = soup.find_all("p")
   for p in all_p:
       print(p.text)
   
   # 按属性查找
   soup.find("p", class_="content")  # 查找 class="content" 的 <p>
   soup.find("a", href="https://example.com")  # 查找 href="https://example.com" 的 <a>
   ```

   

3. select()（CSS选择器）（查询结果以列表形式返回）

   ```python
   # 查找所有p标签
   soup.select("p")
   
   # 查找id="main"
   soup.select("#main")
   
   # 查找class="content"
   soup.select(".content")
   
   # 查找div中的p
   soup.select("div p")
   
   # 获取 <p> 的文本内容
   print(soup.p.text)  # Hello, World!
   
   # 获取 <a> 的 href
   print(soup.a["href"])  # https://example.com
   ```

   

4. 遍历 HTML 结构

   - 获取父标签

     ```python
     print(soup.p.parent)  # 获取 <p> 的父级标签
     ```

   - 获取子标签

     ```python
     for child in soup.body.children:
         print(child)
     ```

   - 获取兄弟标签

     ```python
     print(soup.p.next_sibling)  # 获取下一个兄弟标签
     print(soup.p.previous_sibling)  # 获取上一个兄弟标签
     ```

   

5. 修改和删除

   ```python
   # 修改 <p> 标签内容
   soup.p.string = "你好，世界！"
   print(soup.p)  # <p class="content">你好，世界！</p>
   
   # 删除 <p> 标签
   soup.p.decompose()  
   ```

   